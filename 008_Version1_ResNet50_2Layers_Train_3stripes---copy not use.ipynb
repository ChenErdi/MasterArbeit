{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Custom ResNet\n",
    "___\n",
    "\n",
    "## Table of contents\n",
    "1. [Imports](#Imports)\n",
    "2. [LoadData](#LoadData)\n",
    "3. [Config](#Configs)\n",
    "4. [Analysis](#Analysis)\n",
    "5. [Process](#Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import math\n",
    "import pretrainedmodels as ptm\n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library.datasets import DatasetH5TwoRandom, DatasetH5ForTest, compute_std_mean\n",
    "from Library.transforms import RandomFlip,RandomOffset,RandomRotateGrayscale\n",
    "from Library.scheduler import OneCycleLR,LogLR\n",
    "from Library.cresnet import initialize_cresnet\n",
    "from Library.dml import RefBasedDeepMetric\n",
    "from Library.models.FeatureExtractor import FeatureExtractor, Block, CustomNet\n",
    "from Library.trainers import RefBasedDeepMetricTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Library.util import (\n",
    "    jupyter_wide_screen,\n",
    "    strings_contain_words, \n",
    "    strings_contain_patterns, \n",
    "    add_unique_entry\n",
    ")\n",
    "\n",
    "jupyter_wide_screen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoadData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To initialize dataset: obtain References and Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataset(data_path, fnames):\n",
    "\n",
    "    mean_val = 124.8660 # 6 Seq mean_val\n",
    "    std_val = 67.9694   # 6 Seq std_val\n",
    "    ds_h5 = []\n",
    "    refs_list = []\n",
    "    \n",
    "    ### 可能还需要用 transforms.RandomHorizontalFlip()\n",
    "    #img_transforms = [transforms.ToTensor(),transforms.Normalize((mean_val,),(std_val,))] # 先标准化处理\n",
    "    img_transforms = [RandomFlip(lr_prob=0.5, ud_prob=0.5),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((mean_val,),(std_val,))]\n",
    "    \n",
    "    composed_img_transforms = transforms.Compose(img_transforms)\n",
    "    \n",
    "    comb = os.path.join(data_path, fnames)\n",
    "    # to obtain the 6 Seq data\n",
    "    for i in range(6):\n",
    "        h5_fpath = comb.format(i)\n",
    "        h5_f = DatasetH5TwoRandom(h5_fpath,transform = composed_img_transforms)\n",
    "        \n",
    "        # obtain the references, the first 10 images in every Seq.\n",
    "        refs = h5_f.getRef()\n",
    "        refs_list.append(refs)\n",
    "        \n",
    "        ds_h5.append(h5_f)\n",
    "\n",
    "    return refs_list, ds_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/22008_1000202026_652510007\"\n",
    "fnames = \"roi_versuch2_1_{}_720x20.h5\"\n",
    "refs_list, ds_h5 = init_dataset(data_path,fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader for training set ,validation set, test set (60%,20%,20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 32, 32]\n",
    "shuffle_flags = [True, False, False]\n",
    "ds_ratio = [0.6, 0.2, 0.2]\n",
    "\n",
    "def init_dataloaders(datasets,ds_ratio):\n",
    "    dataloaders = []\n",
    "    \n",
    "    for ds in datasets:\n",
    "        splitted_ds_num_data = [round(len(ds) * ds_ratio[i]) for i in range(len(ds_ratio))]\n",
    "        splitted_ds_num_data[0] += len(ds) - int(np.sum(splitted_ds_num_data))\n",
    "\n",
    "        splitted_ds = []\n",
    "\n",
    "        for tmp_ds, batch_size, shuffle in zip(random_split(ds, splitted_ds_num_data), batch_sizes, shuffle_flags):\n",
    "\n",
    "            splitted_ds.append(\n",
    "                DataLoader(\n",
    "                    tmp_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    drop_last=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        dataloaders.append(splitted_ds)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = init_dataloaders(ds_h5, ds_ratio)\n",
    "\n",
    "train_dls = [dls[0] for dls in dataloaders]\n",
    "valid_dls = [dls[1] for dls in dataloaders]\n",
    "test_dls = [dls[2] for dls in dataloaders]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    # Custom feature extraction model from pretrained ResNet50\n",
    "\n",
    "    def __init__(self, model,zero_init_residual=False):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.conv1= nn.Conv2d(1, 64, 3, 2, 1)\n",
    "        self.bn1 = model.bn1\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        #self.relu = model.relu\n",
    "        self.maxpool = model.maxpool\n",
    "\n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Sequential(nn.Linear(512, 64))\n",
    "                \n",
    "        # weights initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode = 'fan_in', nonlinearity = 'leaky_relu')\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        self.featuremap0 = x.detach() # 核心代码\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        self.featuremap1 = x.detach() # 核心代码\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        self.featuremap2 = x.detach() # 核心代码\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        self.featuremapfc = x.detach() # 核心代码\n",
    "        \n",
    "        # 使用的时候 : feature_output1 = model.featuremap1.transpose(1,0).cpu()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def show_intermediate_shape(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(\"After conv1: \", x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        print(\"After maxpool: \", x.shape)\n",
    "        x = self.layer1(x)\n",
    "        print(\"After layer1: \", x.shape)\n",
    "        x = self.layer2(x)\n",
    "        print(\"After layer2,before avgpool: \", x.shape)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        print(\"After avgpool,before reshape: \", x.shape)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        print(\"After reshape: \", x.shape)\n",
    "        x = self.fc(x)\n",
    "        print(\"In FC: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SiameseNet structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def _calculate_loss(self, v1, v2, refs_vectors):\n",
    "        r\"\"\"Calculate distance between vector of img1&img2 and each reference images\n",
    "\n",
    "        Args:\n",
    "            v1 (Tensor, torch.Size[batchsize,128]): batch of img1s from feature extractor\n",
    "            v2 (Tensor, torch.Size[batchsize,128]): batch of img2s from feature extractor\n",
    "            refs_vectors(List of Tensor, torch.Size([1,128])): List of 10 refs vectors\n",
    "        \"\"\"\n",
    "        v_dim = v1.size(1)\n",
    "        num_ref = len(refs_vectors)\n",
    "\n",
    "        # concatenate the refs tensor\n",
    "        for i,vec in enumerate(refs_vectors):\n",
    "            if i == 0:\n",
    "                refs_tensor = vec\n",
    "            else:\n",
    "                refs_tensor = torch.cat((refs_tensor,vec),dim = 0)\n",
    "\n",
    "\n",
    "        v11 = v1.unsqueeze(1).repeat(1, num_ref, 1)\n",
    "        v22 = v2.unsqueeze(1).repeat(1, num_ref, 1)\n",
    "        refs = refs_tensor.unsqueeze(0).repeat(v1.size(0), 1, 1)  #v1.size(0) = batch size\n",
    "\n",
    "        kernel_matrix1 = (v11-refs).pow(2).sum(2) #torch.Size([batchsize, 10])\n",
    "        kernel_matrix2 = (v22-refs).pow(2).sum(2) #torch.Size([batchsize, 10])\n",
    "\n",
    "        v1_distance_mean = torch.mean(kernel_matrix1,dim = 1) #torch.Size([batchsize])\n",
    "        v2_distance_mean = torch.mean(kernel_matrix2,dim = 1) #torch.Size([batchsize])\n",
    "        \n",
    "        diff = v1_distance_mean - v2_distance_mean\n",
    "        \n",
    "        #print(\"Before Sigmoid outputs\",outputs)\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        outputs = sigmoid(diff)\n",
    "        #print(\"After Sigmoid outputs\",outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x1, x2, refs):\n",
    "        v1 = self.model(x1)\n",
    "        v2 = self.model(x2)\n",
    "        feature_refs = [self.model(ref) for ref in refs]\n",
    "        \n",
    "        outputs = self._calculate_loss(v1, v2, feature_refs)\n",
    "        return outputs\n",
    "    \n",
    "    # This function used to plot the degradation metric , x1 is the sequential image\n",
    "    def forward_test(self, x1, refs):\n",
    "        v1 = self.model(x1)\n",
    "        feature_refs = [self.model(ref) for ref in refs]\n",
    "        \n",
    "        return v1, feature_refs\n",
    "    \n",
    "    def get_model(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RefBasedDeepMetric(\n",
       "  (feature_extractor): FeatureExtractor(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_non_linearity): Sigmoid()\n",
       "  (criterion): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_model = models.resnet50(pretrained=True)\n",
    "feature_extractor = FeatureExtractor(res_model)\n",
    "model = RefBasedDeepMetric(feature_extractor,loss_non_linearity_name='sigmoid',criterion_name='bce',)\n",
    "#model = SiameseNet(feature_extractor)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1:  torch.Size([32, 64, 360, 10])\n",
      "After maxpool:  torch.Size([32, 64, 180, 5])\n",
      "After layer1:  torch.Size([32, 256, 180, 5])\n",
      "After layer2,before avgpool:  torch.Size([32, 512, 90, 3])\n",
      "After avgpool,before reshape:  torch.Size([32, 512, 1, 1])\n",
      "After reshape:  torch.Size([32, 512])\n",
      "In FC:  torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32,1,720,20)\n",
    "model.feature_extractor.show_intermediate_shape(x.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the degradation metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degradation_metrics(v1, refs_vectors):\n",
    "    r\"\"\"Calculate distance between vector of img1&img2 and each reference images\n",
    "    Args:\n",
    "        v1 (Tensor, torch.Size[batchsize,128]): batch of imgs from feature extractor\n",
    "        \n",
    "        refs_vectors(List of Tensor, torch.Size([1,128])): List of 10 refs vectors\n",
    "    \"\"\"\n",
    "    v_dim = v1.size(1) # 128\n",
    "    num_ref = len(refs_vectors)\n",
    "    \n",
    "    # concatenate the refs tensor\n",
    "    for i,vec in enumerate(refs_vectors):\n",
    "        if i == 0:\n",
    "            refs_tensor = vec\n",
    "        else:\n",
    "            refs_tensor = torch.cat((refs_tensor,vec),dim = 0)\n",
    "\n",
    "    v11 = v1.unsqueeze(1).repeat(1, num_ref, 1)\n",
    "    refs = refs_tensor.unsqueeze(0).repeat(v1.size(0), 1, 1)  #v1.size(0) = batch size\n",
    "    \n",
    "    kernel_matrix1 = (v11-refs).pow(2).sum(2) \n",
    "    \n",
    "    metric = torch.mean(kernel_matrix1,dim = 1) \n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch_lr_finder import LRFinder ,TrainDataLoaderIter, ValDataLoaderIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainIter(TrainDataLoaderIter):\n",
    "    def inputs_labels_from_batch(self, batch_data):\n",
    "        img1, img2, labels = batch_data\n",
    "        return img1, img2, labels\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            batch = next(self._iterator)\n",
    "            img1, img2, labels = self.inputs_labels_from_batch(batch)\n",
    "        except StopIteration:\n",
    "            if not self.auto_reset:\n",
    "                raise\n",
    "            self._iterator = iter(self.data_loader)\n",
    "            batch = next(self._iterator)\n",
    "            img1, img2, labels = self.inputs_labels_from_batch(batch)\n",
    "\n",
    "        return img1, img2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomValIter(ValDataLoaderIter):\n",
    "    def inputs_labels_from_batch(self, batch_data):\n",
    "        img1, img2, labels = batch_data\n",
    "        return img1, img2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLR(_LRScheduler):\n",
    "    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float): the final learning rate.\n",
    "        num_iter (int): the number of iterations over which the test occurs.\n",
    "        last_epoch (int, optional): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "\n",
    "        if num_iter <= 1:\n",
    "            raise ValueError(\"`num_iter` must be larger than 1\")\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "        super(LinearLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        r = self.last_epoch / (self.num_iter - 1)\n",
    "\n",
    "        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float): the final learning rate.\n",
    "        num_iter (int): the number of iterations over which the test occurs.\n",
    "        last_epoch (int, optional): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "\n",
    "        if num_iter <= 1:\n",
    "            raise ValueError(\"`num_iter` must be larger than 1\")\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # In earlier Pytorch versions last_epoch starts at -1, while in recent versions\n",
    "        # it starts at 0. We need to adjust the math a bit to handle this. See\n",
    "        # discussion at: https://github.com/davidtvs/pytorch-lr-finder/pull/42\n",
    "        \n",
    "        r = self.last_epoch / (self.num_iter - 1)\n",
    "\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from apex import amp\n",
    "\n",
    "    IS_AMP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    IS_AMP_AVAILABLE = False\n",
    "    \n",
    "class CustomLRFinder(LRFinder):\n",
    "    \n",
    "    def custom_range_test(\n",
    "        self,\n",
    "        train_loader,\n",
    "        curr_refs,\n",
    "        val_loader=None,\n",
    "        start_lr=None,\n",
    "        end_lr=10,\n",
    "        num_iter=100,\n",
    "        step_mode=\"exp\",\n",
    "        smooth_f=0.05,\n",
    "        diverge_th=5,\n",
    "        accumulation_steps=1,\n",
    "        non_blocking_transfer=True,\n",
    "    ):\n",
    "       \n",
    "        # Reset test results\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "\n",
    "        # Move the model to the proper device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Check if the optimizer is already attached to a scheduler\n",
    "        self._check_for_scheduler()\n",
    "\n",
    "        # Set the starting learning rate\n",
    "        if start_lr:\n",
    "            self._set_learning_rate(start_lr)\n",
    "\n",
    "        # Initialize the proper learning rate policy\n",
    "        if step_mode.lower() == \"exp\":\n",
    "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        elif step_mode.lower() == \"linear\":\n",
    "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
    "        else:\n",
    "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
    "\n",
    "        if smooth_f < 0 or smooth_f >= 1:\n",
    "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
    "\n",
    "        # Create an iterator to get data batch by batch\n",
    "        if isinstance(train_loader, DataLoader):\n",
    "            train_iter = TrainDataLoaderIter(train_loader)\n",
    "        elif isinstance(train_loader, TrainDataLoaderIter):\n",
    "            train_iter = train_loader\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`train_loader` has unsupported type: {}.\"\n",
    "                \"Expected types are `torch.utils.data.DataLoader`\"\n",
    "                \"or child of `TrainDataLoaderIter`.\".format(type(train_loader))\n",
    "            )\n",
    "\n",
    "        if val_loader:\n",
    "            if isinstance(val_loader, DataLoader):\n",
    "                val_iter = CustomValIter(val_loader)\n",
    "            elif isinstance(val_loader, ValDataLoaderIter):\n",
    "                val_iter = val_loader\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"`val_loader` has unsupported type: {}.\"\n",
    "                    \"Expected types are `torch.utils.data.DataLoader`\"\n",
    "                    \"or child of `ValDataLoaderIter`.\".format(type(val_loader))\n",
    "                )\n",
    "\n",
    "        for iteration in tqdm(range(num_iter)):\n",
    "            # Train on batch and retrieve loss\n",
    "            loss = self._train_batch(\n",
    "                train_iter,\n",
    "                curr_refs,\n",
    "                accumulation_steps,\n",
    "                non_blocking_transfer= non_blocking_transfer,\n",
    "            )\n",
    "            if val_loader:\n",
    "                loss = self._validate(\n",
    "                    val_iter, curr_refs, non_blocking_transfer=non_blocking_transfer\n",
    "                )\n",
    "\n",
    "            # Update the learning rate\n",
    "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
    "            lr_schedule.step()\n",
    "\n",
    "            # Track the best loss and smooth it if smooth_f is specified\n",
    "            if iteration == 0:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if smooth_f > 0:\n",
    "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "\n",
    "            # Check if the loss has diverged; if it has, stop the test\n",
    "            self.history[\"loss\"].append(loss)\n",
    "            if loss > diverge_th * self.best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "\n",
    "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
    "\n",
    "    def _train_batch(self, train_iter, curr_refs, accumulation_steps, non_blocking_transfer=True):\n",
    "        self.model.train()\n",
    "        total_loss = None  # for late initialization\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.optimizer.zero_grad()\n",
    "        curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "        for i in range(accumulation_steps):\n",
    "            img1, img2, labels = next(train_iter)\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.to(device)\n",
    "            img1 = img1.float().to(device)\n",
    "            img2 = img2.float().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(img1, img2, curr_refs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # Loss should be averaged in each step\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            if IS_AMP_AVAILABLE and hasattr(self.optimizer, \"_amp_stash\"):\n",
    "                # For minor performance optimization, see also:\n",
    "                # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\n",
    "                delay_unscale = ((i + 1) % accumulation_steps) != 0\n",
    "\n",
    "                with amp.scale_loss(\n",
    "                    loss, self.optimizer, delay_unscale=delay_unscale\n",
    "                ) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss.item()\n",
    "    \n",
    "    def _validate(self, val_iter, curr_refs, non_blocking_transfer=True):\n",
    "        # Set model to evaluation mode and disable gradient computation\n",
    "        running_loss = 0\n",
    "        self.model.eval()\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "        with torch.no_grad():\n",
    "            for img1, img2, labels in val_iter:\n",
    "                # Move data to the correct device\n",
    "                labels = labels.view(-1)\n",
    "                labels = labels.to(device)\n",
    "                img1 = img1.float().to(device)\n",
    "                img2 = img2.float().to(device)\n",
    "\n",
    "                # Forward pass and loss computation\n",
    "                outputs = self.model(img1, img2, curr_refs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item() * len(labels)\n",
    "\n",
    "        return running_loss / len(val_iter.dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 1e-8\n",
    "end_lr = 1e-1\n",
    "\n",
    "min_lr_factor   = 0.2\n",
    "anneal_lr_factor= 1e-1\n",
    "\n",
    "train_dl_idx = 0\n",
    "valid_dl_idx = 1\n",
    "test_dl_idx = 2\n",
    "refs_dl_idx = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=start_lr, eps=1e-08, weight_decay=0)\n",
    "#optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.5)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction = 'none')\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder.reset()\n",
    "lr_finder = CustomLRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "\n",
    "# seq dataloader / shuffel the dataloader\n",
    "for refs, train_dl, val_dl in zip (refs_list, train_dls, valid_dls):\n",
    "    custom_train_iter = CustomTrainIter(train_dl)\n",
    "    custom_val_iter = CustomValIter(val_dl)\n",
    "lr_finder.custom_range_test(custom_train_iter, curr_refs = refs, end_lr=end_lr, num_iter=200)\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see the intmediate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_output1 = model.model.featuremap1.transpose(1,0).cpu()\n",
    "feature_output2 = model.model.featuremap2.transpose(1,0).cpu()\n",
    "#feature_output3 = model.model.featuremap3.transpose(1,0).cpu()\n",
    "#feature_output4 = model.model.featuremap4.transpose(1,0).cpu()\n",
    "#feature_output_fc = model.model.featuremapfc.transpose(1,0).cpu()\n",
    "feature_output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_imshow(inp, title=None):\n",
    "    \n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    \n",
    "    inp = inp.detach().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    \n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    \n",
    "    inp = std * inp + mean\n",
    "    \n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(feature_output2)\n",
    "feature_imshow(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "#lr = 3e-4 # bad\n",
    "#lr = 2e-3\n",
    "lr = 1e-3\n",
    "\n",
    "min_lr_factor = 0.1\n",
    "phases_ratio = [0.3, 0.2, 0.3, 0.2]\n",
    "\n",
    "train_dl_idx = 0\n",
    "valid_dl_idx = 1\n",
    "test_dl_idx = 2\n",
    "\n",
    "run_train = True\n",
    "\n",
    "#optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.5)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_train:\n",
    "    writer = SummaryWriter('runs/CResNet_2Layers_Experiment_Type1_7_1e-3')\n",
    "    # 训练迭代总步数\n",
    "    num_train_iter_per_epoch = 0\n",
    "    for seq_dataloaders in dataloaders:\n",
    "        num_train_iter_per_epoch += len(seq_dataloaders[train_dl_idx])\n",
    "        \n",
    "    # 验证迭代总步数 \n",
    "    num_val_iter_per_epoch = 0\n",
    "    for seq_dataloaders in dataloaders:\n",
    "        num_val_iter_per_epoch += len(seq_dataloaders[valid_dl_idx])\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,eps=1e-08)\n",
    "\n",
    "    scheduler = OneCycleLR(optimizer, num_train_iter_per_epoch, num_epochs, min_lr_factor=min_lr_factor)\n",
    "    scheduler.step(epoch=0)\n",
    "    for epoch in tqdm(range(1,num_epochs+1)):\n",
    "\n",
    "        print('Train')\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        running_corrects = 0\n",
    "\n",
    "        train_loss_array = np.zeros(num_train_iter_per_epoch, dtype=np.single)\n",
    "\n",
    "        # mse_y = torch.zeros(batch_sizes[0]//2).to(device)\n",
    "        \n",
    "        i = 0\n",
    "        seq_idx = 1\n",
    "        for (refs, train_dl) in zip(refs_list,train_dls):\n",
    "            # obtain the refs in every seq\n",
    "            \n",
    "            curr_train_dl = train_dl\n",
    "            curr_refs = refs\n",
    "            curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "\n",
    "            # Training\n",
    "            dl_iter = iter(curr_train_dl)\n",
    "            \n",
    "            # This two only used to compute the loss and acc in every Sequence\n",
    "            running_loss = 0\n",
    "            corrects = 0\n",
    "            for idx in tqdm(range(len(curr_train_dl))):\n",
    "                try:\n",
    "                    img1, img2, targets = next(dl_iter)\n",
    "\n",
    "                except (StopIteration, TypeError):\n",
    "                    dl_iter = iter(curr_train_dl)\n",
    "                    img1, img2, targets = next(dl_iter)\n",
    "                \n",
    "                scheduler.zero_grad()\n",
    "                \n",
    "                img1 = img1.float().to(device)\n",
    "                img2 = img2.float().to(device)\n",
    "                targets = targets.view(-1)\n",
    "                targets = targets.float().to(device)\n",
    "\n",
    "                # forward pass\n",
    "                outputs = model(img1,img2, curr_refs)\n",
    "                preds = torch.gt(outputs,0.5).float()\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                corrects += torch.sum(preds == targets)\n",
    "                running_corrects += torch.sum(preds == targets)\n",
    "    \n",
    "                # every 100th print information about batch\n",
    "                if idx % int(num_train_iter_per_epoch/5) == 0:\n",
    "                    print(f'epoch: {epoch}/{num_epochs}, step: {idx+1}/{len(curr_train_dl)},targets: {targets},Outputs: {outputs}, Loss:{loss.item()}')\n",
    "\n",
    "                train_loss_array[i] = loss.cpu().detach().numpy()\n",
    "                \n",
    "                # record loss and accuracy per iteration\n",
    "                c = torch.sum(preds == targets)\n",
    "                acc = int(c) / img1.size(0)\n",
    "                writer.add_scalar('Train/Loss per Iteration',train_loss_array[i], ((epoch - 1) * num_train_iter_per_epoch)+ i)\n",
    "                writer.add_scalar('Train/Acc per Iteration',acc, ((epoch - 1) * num_train_iter_per_epoch)+ i)\n",
    "                \n",
    "                # Record the lr\n",
    "                curr_lr = get_lr(optimizer)\n",
    "                writer.add_scalar('Train/Learning Rate',curr_lr, ((epoch - 1) * num_train_iter_per_epoch)+ i)\n",
    "                \n",
    "                i += 1\n",
    "                scheduler.step()\n",
    "                \n",
    "            # record loss and accuracy in every train dataloader (namely every Sequence)\n",
    "            seq_train_mean_loss = running_loss / len(curr_train_dl)\n",
    "            seq_train_acc = int(corrects) / (len(curr_train_dl) * img1.size(0))\n",
    "            print(\"Sequence:{}, Seq Training Mean Loss: {}, Seq Training Acc: {}\".format(seq_idx, seq_train_mean_loss, seq_train_acc))\n",
    "            print(\"******************************************************************\")\n",
    "            # Record the loss and acc in every Seq in Tensorboard\n",
    "            writer.add_scalar('Train/Seq Training Mean Loss',seq_train_mean_loss, (epoch - 1) * len(train_dls) + seq_idx)\n",
    "            writer.add_scalar('Train/Seq Training Accuracy', seq_train_acc, (epoch - 1) * len(train_dls) + seq_idx)\n",
    "            writer.add_scalar('Train/Seq Error Rate',1 - seq_train_acc, (epoch - 1) * len(train_dls) + seq_idx)\n",
    "            \n",
    "            seq_idx += 1\n",
    "            \n",
    "        # after one epoch update scheduler   \n",
    "        # Plot lr policy\n",
    "        curr_lr = get_lr(optimizer)\n",
    "        writer.add_scalar('Train/Learning Rate Policy',curr_lr , epoch)\n",
    "        writer.flush()\n",
    "            \n",
    "        # record loss and accuracy in every epoch (namely 6 Sequences)\n",
    "        epoch_train_mean_loss = train_loss_array.mean()  # std\n",
    "        epoch_train_acc = int(running_corrects) / (num_train_iter_per_epoch * img1.size(0))\n",
    "\n",
    "        print(\"Epoch:{}, Epoch Training Loss Mean: {}, Epoch Acc: {}\".format(epoch, epoch_train_mean_loss, epoch_train_acc))\n",
    "        print(\"******************************************************************\")\n",
    "        \n",
    "        ### Validation\n",
    "        print('Validation')\n",
    "        model.eval()\n",
    "        val_loss_array = np.zeros(num_val_iter_per_epoch, dtype=np.single)\n",
    "        \n",
    "        running_corrects = 0\n",
    "\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "            seq_idx = 1\n",
    "            for (refs, valid_dl) in zip(refs_list,valid_dls):\n",
    "                \n",
    "                curr_valid_dl = valid_dl\n",
    "                curr_refs = refs\n",
    "                curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "            \n",
    "                dl_iter = iter(curr_valid_dl)\n",
    "                # This two uesd to compute loss and accurary in every Seq\n",
    "                running_loss = 0\n",
    "                corrects = 0\n",
    "                for dl_idx in tqdm(range(len(curr_valid_dl))):\n",
    "                    try:\n",
    "                        img1, img2, targets = next(dl_iter)\n",
    "                    except (StopIteration, TypeError):\n",
    "                        dl_iter = iter(curr_valid_dl)\n",
    "                        img1, img2, targets = next(dl_iter)\n",
    "                    \n",
    "                    scheduler.zero_grad()\n",
    "\n",
    "                    img1 = img1.float().to(device)\n",
    "                    img2 = img2.float().to(device)\n",
    "                    targets = targets.view(-1)\n",
    "                    targets = targets.float().to(device)\n",
    "                \n",
    "                    outputs = model(img1,img2, curr_refs)\n",
    "                    preds = torch.gt(outputs,0.5).float()\n",
    "                    \n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    corrects += torch.sum(preds == targets)\n",
    "                    running_corrects += torch.sum(preds == targets)\n",
    "                    \n",
    "                    # record loss and accuracy in every iteration\n",
    "                    c = torch.sum(preds == targets)\n",
    "                    acc = int(c) / img1.size(0)\n",
    "                    writer.add_scalar('Validation/Loss per Iteration',loss.item(), ((epoch - 1) * num_val_iter_per_epoch)+ i)\n",
    "                    writer.add_scalar('Validation/Acc per Iteration',acc, ((epoch - 1) * num_val_iter_per_epoch)+ i)\n",
    "\n",
    "                    val_loss_array[i] = loss.cpu().detach().numpy()\n",
    "                    i += 1\n",
    "            \n",
    "                seq_val_mean_loss = running_loss / len(curr_valid_dl)\n",
    "                seq_val_acc = int(corrects) / (len(curr_valid_dl) * img1.size(0))\n",
    "                print(\"******************************************************************\")\n",
    "                print(\"Sequence:{}, Seq Mean Loss: {}, Seq Val Acc: {}\".format(seq_idx,seq_val_mean_loss, seq_val_acc))\n",
    "                \n",
    "                writer.add_scalar('Validation/Seq Mean Loss',seq_val_mean_loss, (epoch - 1) * len(valid_dls) + seq_idx)\n",
    "                writer.add_scalar('Validation/Seq Accuracy', seq_val_acc, (epoch - 1) * len(valid_dls) + seq_idx)\n",
    "                writer.add_scalar('Validation/Seq Error Rate',1 - seq_val_acc, (epoch - 1) * len(valid_dls) + seq_idx)\n",
    "                \n",
    "                seq_idx += 1\n",
    "                \n",
    "            epoch_val_mean_loss = val_loss_array.mean()  # std\n",
    "            epoch_val_acc = int(running_corrects) / (num_val_iter_per_epoch * img1.size(0))\n",
    "            \n",
    "            print(\"******************************************************************\")\n",
    "            print(\"Epoch:{}, Epoch Val Mean Loss: {}, Epoch Val Acc: {}\".format(epoch, epoch_val_mean_loss, epoch_val_acc))\n",
    "            print(\"******************************************************************\")\n",
    "        \n",
    "        # Tensorboard\n",
    "        writer.add_scalars('Epoch/Loss', {\"Training Loss\": epoch_train_mean_loss,\n",
    "                                          \"Validation Loss\": epoch_val_mean_loss}, epoch)\n",
    "        writer.add_scalars('Epoch/Accuracy', {\"Training Accuracy\":epoch_train_acc,\n",
    "                                              \"Validation Accuracy\": epoch_val_acc}, epoch)\n",
    "        writer.add_scalars('Epoch/Error Rate', {\"Training\": 1 - epoch_train_acc,\n",
    "                                                \"Validation\": 1 - epoch_val_acc}, epoch)\n",
    "        writer.flush()\n",
    "        \n",
    "        plt.plot(np.linspace(0, 1, train_loss_array.size), train_loss_array,label='Training Loss')\n",
    "        plt.plot(np.linspace(0, 1, val_loss_array.size), val_loss_array, label='Validation Loss')\n",
    "        \n",
    "        train_parting_line = [37/580, 82/580, 209/580, 291/580, 476/580] \n",
    "        val_parting_line = [12/191,27/191,69/191,96/191,157/191]\n",
    "        plt.vlines(train_parting_line, 0, 1, colors = \"r\", linestyles = \"dashed\")\n",
    "        plt.vlines(val_parting_line, 0, 1, colors = \"r\", linestyles = \"dashed\")\n",
    "        \n",
    "        plt.title('Epoch {}'.format(epoch))\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "    # add model to tensorboard\n",
    "    writer.add_graph(model,(img1,img2,curr_refs,))\n",
    "    writer.flush()   \n",
    "        \n",
    "    print('Finished Training')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_test_iter_per_epoch = 0\n",
    "for seq_dataloaders in dataloaders:\n",
    "    num_test_iter_per_epoch += len(seq_dataloaders[test_dl_idx])\n",
    "    \n",
    "model.eval()\n",
    "y_vect = np.zeros(batch_size * num_test_iter_per_epoch)\n",
    "y_tilde_vect = np.zeros(batch_size * num_test_iter_per_epoch)\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    seq_idx = 1\n",
    "    \n",
    "    running_corrects = 0\n",
    "\n",
    "    test_loss_array = np.zeros(num_test_iter_per_epoch, dtype=np.single)\n",
    "    \n",
    "    for (refs, test_dl) in zip(refs_list,test_dls):\n",
    "        \n",
    "        curr_test_dl = test_dl\n",
    "        curr_refs = refs\n",
    "        curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "\n",
    "        dl_iter = iter(curr_test_dl)\n",
    "        \n",
    "        # This two uesd to compute loss and accurary in every Seq\n",
    "        running_loss = 0\n",
    "        corrects = 0\n",
    "\n",
    "        for dl_idx in tqdm(range(len(curr_test_dl))):\n",
    "            try:\n",
    "                img1, img2, targets = next(dl_iter)\n",
    "            except (StopIteration, TypeError):\n",
    "                dl_iter = iter(curr_valid_dl)\n",
    "                img1, img2, targets = next(dl_iter)\n",
    "\n",
    "            scheduler.zero_grad()\n",
    "            img1 = img1.float().to(device)\n",
    "            img2 = img2.float().to(device)\n",
    "            targets = targets.view(-1)\n",
    "            targets = targets.float().to(device)\n",
    "            \n",
    "            outputs = model(img1,img2, curr_refs)\n",
    "            preds = torch.gt(outputs,0.5).float()\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            corrects += torch.sum(preds == targets)\n",
    "            \n",
    "            test_loss_array[i] = loss.cpu().detach().numpy()\n",
    "\n",
    "            y_vect[i*batch_size:(i+1)*batch_size] = targets.detach().cpu().numpy()\n",
    "            y_tilde_vect[i*batch_size:(i+1)*batch_size] = outputs.detach().cpu().numpy()\n",
    "            \n",
    "            # record loss and accuracy per iteration\n",
    "            c = torch.sum(preds == targets)\n",
    "            acc = int(c) / img1.size(0)\n",
    "            \n",
    "            writer.add_scalar('Test/Loss per Iteration',test_loss_array[i], i)\n",
    "            writer.add_scalar('Test/Acc per Iteration',acc, i)\n",
    "            writer.add_scalar('Test/Error Rate', 1 - acc, i)\n",
    "\n",
    "            i += 1\n",
    "        # record seq test loss and acc\n",
    "        seq_test_mean_loss = running_loss / len(curr_test_dl)\n",
    "        seq_test_acc = int(corrects) / (len(curr_test_dl) * img1.size(0))\n",
    "        \n",
    "        print(\"******************************************************************\")\n",
    "        print(\"Test: Sequence:{}, Seq Mean Loss: {}, Seq Acc: {}\".format(seq_idx,seq_test_mean_loss, seq_test_acc))\n",
    "            \n",
    "        writer.add_scalar('Test/Seq Loss', seq_test_mean_loss, seq_idx)\n",
    "        writer.add_scalar('Test/Seq Accuracy', seq_test_acc, seq_idx)\n",
    "        writer.add_scalar('Test/Seq Error Rate', 1 - seq_test_acc, seq_idx)\n",
    "        \n",
    "        seq_idx += 1\n",
    "\n",
    "writer.close()\n",
    "                                      \n",
    "print(\"Test finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_vect)\n",
    "plt.plot(y_tilde_vect[np.argsort(y_vect)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Build DatasetH5 for test\n",
    "\n",
    "mean_val = 124.8660 # 6 Seq mean_val\n",
    "std_val = 67.9694   # 6 Seq std_val\n",
    "\n",
    "img_transforms = [RandomFlip(lr_prob=0.5, ud_prob=0.5),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((mean_val,),(std_val,))]\n",
    "composed_img_transforms = transforms.Compose(img_transforms)\n",
    "\n",
    "im0_transforms = [\n",
    "    #RandomFlip(lr_prob=0.5, ud_prob=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([mean_val], [255]),\n",
    "]\n",
    "im0_transforms = transforms.Compose(im0_transforms)\n",
    "\n",
    "im1_transforms = [\n",
    "    RandomFlip(lr_prob=0, ud_prob=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([mean_val], [255]),\n",
    "]\n",
    "im1_transforms = transforms.Compose(im1_transforms)\n",
    "\n",
    "im2_transforms = [\n",
    "    RandomFlip(lr_prob=1, ud_prob=0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([mean_val], [255]),\n",
    "]\n",
    "im2_transforms = transforms.Compose(im2_transforms)\n",
    "\n",
    "im3_transforms = [\n",
    "    RandomFlip(lr_prob=1, ud_prob=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([mean_val], [255]),\n",
    "]\n",
    "im3_transforms = transforms.Compose(im3_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init test dataset for plotting degradation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_test_dataset(data_path, fnames):\n",
    "\n",
    "    mean_val = 124.8660 # 6 Seq mean_val\n",
    "    std_val = 67.9694   # 6 Seq std_val\n",
    "    ds_h5_test = []\n",
    "    refs_list = []\n",
    "    \n",
    "    ### 可能还需要用 transforms.RandomHorizontalFlip()\n",
    "    #img_transforms = [transforms.ToTensor(),transforms.Normalize((mean_val,),(std_val,))] # 先标准化处理\n",
    "    img_transforms = [RandomFlip(lr_prob=0.5, ud_prob=0.5),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((mean_val,),(std_val,))]\n",
    "    \n",
    "    composed_img_transforms = transforms.Compose(img_transforms)\n",
    "    \n",
    "    comb = os.path.join(data_path, fnames)\n",
    "    # to obtain the 6 Seq data\n",
    "    for i in range(6):\n",
    "        h5_fpath = comb.format(i)\n",
    "        h5_f_test = DatasetH5ForTest(h5_fpath,normalize_im=False, transform = composed_img_transforms)\n",
    "\n",
    "        # obtain the references, the first 10 images in every Seq.\n",
    "        refs = h5_f_test.getRef()\n",
    "        refs_list.append(refs)\n",
    "\n",
    "        ds_h5_test.append(h5_f_test)\n",
    "    \n",
    "    return refs_list, ds_h5_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/22008_1000202026_652510007\"\n",
    "fnames = \"roi_versuch2_1_{}_720x20.h5\"\n",
    "refs_list, ds_h5_test = init_test_dataset(data_path,fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_h5_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "test_dataloaders = []\n",
    "for ds in ds_h5_test: \n",
    "    test_dataloader = DataLoader(dataset = ds, batch_size=batch_size, shuffle = False)\n",
    "    test_dataloaders.append(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degradation metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degradation_metrics(v1, refs_vectors):\n",
    "    r\"\"\"Calculate distance between vector of img1&img2 and each reference images\n",
    "    Args:\n",
    "        v1 (Tensor, torch.Size[batchsize,128]): batch of imgs from feature extractor\n",
    "        \n",
    "        refs_vectors(List of Tensor, torch.Size([1,128])): List of 10 refs vectors\n",
    "    \"\"\"\n",
    "    v_dim = v1.size(1) # 128\n",
    "    num_ref = len(refs_vectors)\n",
    "    \n",
    "    # concatenate the refs tensor\n",
    "    for i,vec in enumerate(refs_vectors):\n",
    "        if i == 0:\n",
    "            refs_tensor = vec\n",
    "        else:\n",
    "            refs_tensor = torch.cat((refs_tensor,vec),dim = 0)\n",
    "\n",
    "    v11 = v1.unsqueeze(1).repeat(1, num_ref, 1)\n",
    "    refs = refs_tensor.unsqueeze(0).repeat(v1.size(0), 1, 1)  #v1.size(0) = batch size\n",
    "    \n",
    "    kernel_matrix1 = (v11-refs).pow(2).sum(2) \n",
    "    \n",
    "    metric = torch.mean(kernel_matrix1,dim = 1) \n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq_y_tilde_vect = np.zeros(0)\n",
    "time_ids = []\n",
    "\n",
    "for refs,dl in zip(refs_list, test_dataloaders):\n",
    "\n",
    "    curr_refs = refs\n",
    "    curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "    \n",
    "    num_data = len(dl)\n",
    "    print(num_data)\n",
    "    \n",
    "    loss_vect = np.zeros(num_data)\n",
    "    var_res_vect= np.zeros(num_data)\n",
    "\n",
    "    y_vect = np.zeros(num_data)\n",
    "    y_tilde_vect = np.zeros(num_data)\n",
    "    \n",
    "    dl_iter = iter(dl)\n",
    "\n",
    "    for i in tqdm(range(num_data)):\n",
    "        try:\n",
    "            img,label = next(dl_iter)\n",
    "\n",
    "        except (StopIteration, TypeError):\n",
    "            dl_iter = iter(curr_train_dl)\n",
    "            img,label = next(dl_iter)\n",
    "            \n",
    "        y_vect[i] = label\n",
    "        \n",
    "        img = img.float().to(device)\n",
    "                \n",
    "        #targets = targets.float().to(device)\n",
    "        \n",
    "        v1, refs_v = model.forward_test(img, curr_refs)\n",
    "        metric = degradation_metrics(v1,refs_v)\n",
    "        \n",
    "        y_tilde = metric.detach().cpu().numpy()\n",
    "\n",
    "        var_res_vect[i] = np.var(y_tilde)\n",
    "\n",
    "        y_tilde_vect[i] = y_tilde.mean()\n",
    "\n",
    "        loss_vect[i] = np.abs(label - y_tilde_vect[i])\n",
    "        \n",
    "    time_ids.append(y_tilde_vect.size)\n",
    "        \n",
    "    all_seq_y_tilde_vect = np.concatenate((all_seq_y_tilde_vect, y_tilde_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(all_seq_y_tilde_vect, label='Degradation Value')\n",
    "plt.plot(pd.Series(all_seq_y_tilde_vect).rolling(18).mean(), label='Mean (winsize:32)')\n",
    "\n",
    "min_y = all_seq_y_tilde_vect.min()\n",
    "max_y = all_seq_y_tilde_vect.max()\n",
    "\n",
    "#curr_x_pos = 0\n",
    "#for i in range(1, len(im_ds_h5)):\n",
    "#    print(len(im_ds_h5[i]))\n",
    "#    curr_x_pos += len(im_ds_h5[i])\n",
    "#    plt.plot([curr_x_pos, curr_x_pos], [min_y, max_y])\n",
    "'''\n",
    "seq_period_iloc = [\n",
    "    [    10,  2000],\n",
    "    #[ 2001,  2265],\n",
    "    [ 2001,  4723],\n",
    "    [ 4751, 11579],\n",
    "    [11585, 16018],\n",
    "    [16042, 25962],\n",
    "    [25962, 31000],\n",
    "]\n",
    "'''\n",
    "seq_period_iloc = [\n",
    "    [    0,  1991],\n",
    "    [ 1991,  4440],\n",
    "    [ 4440, 11259],\n",
    "    [11259, 15683],\n",
    "    [15683, 20914],\n",
    "    [20914, 25594],\n",
    "    #[15683, 25594],\n",
    "    [25594, 31170],\n",
    "]\n",
    "\n",
    "seq_index = 1\n",
    "for stime_iloc, etime_iloc in seq_period_iloc:\n",
    "    plt.plot([stime_iloc, stime_iloc], [min_y, max_y], label = \"start of Seq{}\".format(seq_index))\n",
    "    plt.plot([etime_iloc, etime_iloc], [min_y, max_y])\n",
    "    seq_index += 1\n",
    "\n",
    "\n",
    "plt.ylabel(\"Degradation (Unitless)\")\n",
    "plt.xlabel(\"TimeIndex\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.figure(figsize = (20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "plot_raw_pts = True\n",
    "    \n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(all_seq_y_tilde_vect.size),\n",
    "        y=all_seq_y_tilde_vect,\n",
    "        mode='lines',\n",
    "        #mode='lines+markers',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(all_seq_y_tilde_vect.size),\n",
    "        y=pd.Series(all_seq_y_tilde_vect).rolling(32).mean(),\n",
    "        mode='lines',\n",
    "        #mode='lines+markers',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_seq_y_tilde_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Use //// Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, dataloaders, optimizer, num_epochs):\n",
    "    \n",
    "    best_acc = 0.\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # 训练迭代总步数\n",
    "    num_train_iter_per_epoch = 0\n",
    "    for seq_dataloaders in dataloaders:\n",
    "        num_train_iter_per_epoch += len(seq_dataloaders[train_dl_idx])\n",
    "    \n",
    "    # 验证迭代总步数 \n",
    "    num_valid_iter_per_epoch = 0\n",
    "    for seq_dataloaders in dataloaders:\n",
    "        num_valid_iter_per_epoch += len(seq_dataloaders[valid_dl_idx])\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        running_loss = 0.\n",
    "        running_corrects = 0.\n",
    "\n",
    "        print('Train')\n",
    "        model.train()\n",
    "\n",
    "        train_loss_array = np.zeros(num_train_iter_per_epoch, dtype=np.single)\n",
    "        \n",
    "        for (refs, train_dl) in zip(refs_list,train_dls):\n",
    "            # obtain the refs in every seq\n",
    "            curr_train_dl = train_dl\n",
    "            curr_refs = refs\n",
    "            curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "\n",
    "            # Training\n",
    "            dl_iter = iter(curr_train_dl)\n",
    "\n",
    "            for idx in tqdm(range(len(curr_train_dl))):\n",
    "                try:\n",
    "                    img1, img2, targets = next(dl_iter)\n",
    "\n",
    "                except (StopIteration, TypeError):\n",
    "                    dl_iter = iter(curr_train_dl)\n",
    "                    img1, img2, targets = next(dl_iter)            \n",
    "\n",
    "                img1 = img1.float().to(device)\n",
    "                img2 = img2.float().to(device)\n",
    "                #targets = targets.view(-1)\n",
    "                targets = targets.float().to(device)\n",
    "                #print(\"targets shape:\",targets.shape)\n",
    "                #print(\"targets: \",targets)\n",
    "\n",
    "                # forward pass\n",
    "                feature_vector1, feature_vector2, feature_refs = model(img1,img2,curr_refs)\n",
    "                #print(\"feature_vector1 shape:\",feature_vector1.shape)\n",
    "\n",
    "                outputs = degradation_metrics(feature_vector1, feature_vector2, feature_refs)\n",
    "                \n",
    "                #print(\"outputs: \",outputs)# tensor\n",
    "                #print(\"outputs shape: \",outputs.shape)\n",
    "            \n",
    "                loss = criterion(outputs, targets)\n",
    "                preds = nn.outputs.argmax(dim = 1)\n",
    "                \n",
    "                #mean_loss = sum(loss)/len(loss)\n",
    "                # backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * img1.size(0)\n",
    "                running_corrects += torch.sum(preds.view(-1) == targets.view(-1).item())\n",
    "                # every 8th print information about batch\n",
    "                if (idx+1) % 10 == 0:\n",
    "                    print(f'epoch: {epoch + 1}/{num_epochs}, step: {idx+1}/{len(curr_train_dl)},targets: {targets},outputs: {outputs}, Loss:{loss.item()}')\n",
    "\n",
    "            epoch_loss = running_loss / len(curr_train_dl)\n",
    "            epoch_acc = running_corrects / len(curr_train_dl)\n",
    "\n",
    "            print(\"Epoch Loss: {}, Epoch Acc: {}\".format(epoch_loss, epoch_acc))\n",
    "            print(\"******************************************************************\")\n",
    "            \n",
    "        ### Validation\n",
    "        print('Validation')\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_array = np.zeros(num_valid_iter_per_epoch, dtype=np.single)\n",
    "\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for (refs, valid_dl) in zip(refs_list,valid_dls):\n",
    "\n",
    "                curr_valid_dl = valid_dl\n",
    "                curr_refs = refs\n",
    "                curr_refs = [ref.float().to(device) for ref in curr_refs]\n",
    "            \n",
    "                dl_iter = iter(curr_valid_dl)\n",
    "\n",
    "                for dl_idx in tqdm(range(len(curr_valid_dl))):\n",
    "                    try:\n",
    "                        img1, img2, targets = next(dl_iter)\n",
    "                    except (StopIteration, TypeError):\n",
    "                        dl_iter = iter(curr_valid_dl)\n",
    "                        img1, img2, targets = next(dl_iter)\n",
    "                    \n",
    "                    img1 = img1.float().to(device)\n",
    "                    img2 = img2.float().to(device)\n",
    "                    targets = targets.view(-1)\n",
    "                    targets = targets.float().to(device)\n",
    "                    #print(\"targets shape:\",targets.shape)\n",
    "                    #print(\"targets: \",targets)\n",
    "\n",
    "                    # forward pass\n",
    "                    feature_vector1, feature_vector2, feature_refs = model(img1,img2, curr_refs)\n",
    "                    #print(\"feature_vector1 shape:\",feature_vector1.shape)\n",
    "\n",
    "                    outputs = degradation_metrics(feature_vector1, feature_vector2, feature_refs)\n",
    "\n",
    "                    #print(\"outputs: \",outputs)# tensor\n",
    "                    #print(\"outputs shape: \",outputs.shape)\n",
    "\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    preds = outputs.argmax(dim = 1)\n",
    "                    \n",
    "                    running_loss += loss.item() * img1.size(0)\n",
    "                    running_corrects += torch.sum(preds.view(-1) == targets.view(-1).item())\n",
    "                    \n",
    "                    # every 10th print information about batch\n",
    "                    if (idx+1) % 10 == 0:\n",
    "                        print(f'epoch: {epoch + 1}/{num_epochs}, step: {idx+1}/{len(curr_train_dl)},targets: {targets}, outputs: {outputs}, Loss:{loss.item()}')\n",
    "\n",
    "                epoch_loss = running_loss / len(curr_train_dl)\n",
    "                epoch_acc = running_corrects / len(curr_train_dl)\n",
    "\n",
    "                print(\"Epoch Loss: {}, Epoch Acc: {}\".format(epoch_loss, epoch_acc))\n",
    "        \n",
    "            \n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_iter_per_epoch = 0\n",
    "for seq_dataloaders in dataloaders:\n",
    "    num_train_iter_per_epoch += len(seq_dataloaders[train_dl_idx])\n",
    "    print(\"num_train_iter_per_epoch: \",num_train_iter_per_epoch)\n",
    "# 验证迭代总步数 \n",
    "num_valid_iter_per_epoch = 0\n",
    "for seq_dataloaders in dataloaders:\n",
    "    num_valid_iter_per_epoch += len(seq_dataloaders[valid_dl_idx])\n",
    "    print(\"num_valid_iter_per_epoch\",num_valid_iter_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
